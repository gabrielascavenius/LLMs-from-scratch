{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:20:45.676410Z",
     "start_time": "2025-04-24T20:20:45.671024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Chapter 5:\n",
    "# Your Gabriela Scavenius\n",
    "# Date: April 24, 2025"
   ],
   "id": "cbb9521208a57055",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chapter 5 Exercise solutions",
   "id": "188b98b7a25f6331"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:20:45.713356Z",
     "start_time": "2025-04-24T20:20:45.700417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ],
   "id": "ebed75bc1ac2cb74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 2.0.2\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.6.0\n",
      "tensorflow version: 2.19.0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exercise 5.1: Temperature-scaled softmax scores and sampling probabilities",
   "id": "453527c04000e934"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- We can print the number of times the word \"pizza\" is sampled using the `print_sampled_tokens` function we defined in this section\n",
    "- Let's start with the code we defined in section 5.3.1\n",
    "\n",
    "- It is sampled 0x if the temperature is 0 or 0.1, and it is sampled 32x if the temperature is scaled up to 5. The estimated probability is 32/1000 * 100% = 3.2%\n",
    "\n",
    "- The actual probability is 4.3% and contained in the rescaled softmax probability tensor (`scaled_probas[2][6]`)"
   ],
   "id": "43f05ca2e82b473e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Below is a self-contained example using code from chapter 5:",
   "id": "d4231bc578648219"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:20:45.731175Z",
     "start_time": "2025-04-24T20:20:45.719451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "\n",
    "temperatures = [1, 0.1, 5]  # Original, higher, and lower temperature\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ],
   "id": "539ab38bceec2d48",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Now, we can iterate over the `scaled_probas` and print the sampling frequencies in each case:",
   "id": "f840769cb97c0965"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:20:45.801295Z",
     "start_time": "2025-04-24T20:20:45.750816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, probas in enumerate(scaled_probas):\n",
    "    print(\"\\n\\nTemperature:\", temperatures[i])\n",
    "    print_sampled_tokens(probas)"
   ],
   "id": "7171ef9c87dfc858",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Temperature: 1\n",
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n",
      "\n",
      "\n",
      "Temperature: 0.1\n",
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "985 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "15 x toward\n",
      "\n",
      "\n",
      "Temperature: 5\n",
      "165 x closer\n",
      "75 x every\n",
      "42 x effort\n",
      "239 x forward\n",
      "71 x inches\n",
      "46 x moves\n",
      "32 x pizza\n",
      "227 x toward\n",
      "103 x you\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Note that sampling offers an approximation of the actual probabilities when the word \"pizza\" is sampled\n",
    "- E.g., if it is sampled 32/1000 times, the estimated probability is 3.2%\n",
    "- To obtain the actual probability, we can check the probabilities directly by accessing the corresponding entry in `scaled_probas`\n",
    "\n",
    "- Since \"pizza\" is the 7th entry in the vocabulary, for the temperature of 5, we obtain it as follows:"
   ],
   "id": "1decdbce794630d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:33:12.143885Z",
     "start_time": "2025-04-24T20:33:12.127377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "temp5_idx = 2\n",
    "pizza_idx = 6\n",
    "\n",
    "scaled_probas[temp5_idx][pizza_idx]"
   ],
   "id": "ce8d598c2d4b6ab1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0430)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There is a 4.3% probability that the word \"pizza\" is sampled if the temperature is set to 5",
   "id": "b1cb509f25ed042e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exercise 5.2: Different temperature and top-k settings",
   "id": "da3a4a2f010434db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Both temperature and top-k settings have to be adjusted based on the individual LLM (a kind of trial and error process until it generates desirable outputs)\n",
    "- The desirable outcomes are also application-specific, though\n",
    "  - Lower top-k and temperatures result in less random outcomes, which is desired when creating educational content, technical writing or question answering, data analyses, code generation, and so forth\n",
    "  - Higher top-k and temperatures result in more diverse and random outputs, which is more desirable for brainstorming tasks, creative writing, and so forth"
   ],
   "id": "9a53f9c341d7dfda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exercise 5.3: Deterministic behavior in the decoding functions",
   "id": "630c24175eb43702"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There are multiple ways to force deterministic behavior with the `generate` function:\n",
    "\n",
    "1. Setting to `top_k=None` and applying no temperature scaling;\n",
    "2. Setting `top_k=1`."
   ],
   "id": "7638361d5d2121f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Below is a self-contained example using code from chapter 5:",
   "id": "76eb6f9e2b6f9f61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:37:23.303943Z",
     "start_time": "2025-04-24T20:37:22.336744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 256,       # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,       # Embedding dimension\n",
    "    \"n_heads\": 12,        # Number of attention heads\n",
    "    \"n_layers\": 12,       # Number of layers\n",
    "    \"drop_rate\": 0.1,     # Dropout rate\n",
    "    \"qkv_bias\": False     # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
    "model.eval();"
   ],
   "id": "3613c5cb886628cd",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 21\u001B[39m\n\u001B[32m     19\u001B[39m tokenizer = tiktoken.get_encoding(\u001B[33m\"\u001B[39m\u001B[33mgpt2\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     20\u001B[39m model = GPTModel(GPT_CONFIG_124M)\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m model.load_state_dict(\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodel.pth\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n\u001B[32m     22\u001B[39m model.eval();\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spring 2025/CPSC352_AI/LLMs-from-scratch/.venv/lib/python3.11/site-packages/torch/serialization.py:1425\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[39m\n\u001B[32m   1422\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args.keys():\n\u001B[32m   1423\u001B[39m     pickle_load_args[\u001B[33m\"\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1425\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[32m   1426\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[32m   1427\u001B[39m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[32m   1428\u001B[39m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[32m   1429\u001B[39m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[32m   1430\u001B[39m         orig_position = opened_file.tell()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spring 2025/CPSC352_AI/LLMs-from-scratch/.venv/lib/python3.11/site-packages/torch/serialization.py:751\u001B[39m, in \u001B[36m_open_file_like\u001B[39m\u001B[34m(name_or_buffer, mode)\u001B[39m\n\u001B[32m    749\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[32m    750\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[32m--> \u001B[39m\u001B[32m751\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    752\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    753\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mw\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spring 2025/CPSC352_AI/LLMs-from-scratch/.venv/lib/python3.11/site-packages/torch/serialization.py:732\u001B[39m, in \u001B[36m_open_file.__init__\u001B[39m\u001B[34m(self, name, mode)\u001B[39m\n\u001B[32m    731\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[32m--> \u001B[39m\u001B[32m732\u001B[39m     \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'model.pth'"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:32:12.779879Z",
     "start_time": "2025-04-24T20:32:00.143772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt_generate import generate, text_to_token_ids, token_ids_to_text\n",
    "from previous_chapters import generate_text_simple"
   ],
   "id": "7304604650f3f644",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:38:43.762738Z",
     "start_time": "2025-04-24T20:38:41.907252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deterministic function that used torch.argmax\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "id": "76c62eb0dcc9c324",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Samoa parad Defensive MacBook Referospace preparation Einstein ShepherdMot gadgets insurgent dominanceFree Lore 206 Delayitteneda1100alore LIN Factor esteemed Dawn\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:39:21.143358Z",
     "start_time": "2025-04-24T20:39:19.536393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deterministic behavior: No top_k, no temperature scaling\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=None,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "id": "2d928ee44fa7ef2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you646 paraduxe currentlyarget rebuilding Fortune Jin MLS improvised Raiders notions bonus Keeper sur considerablyJohnnyrequire183Randomtc inhibitionidia Conservosaurs\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Note that re-executing the previous code cell will produce the exact same generated text:",
   "id": "895ec25f2ec4c763"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:40:01.115875Z",
     "start_time": "2025-04-24T20:39:59.472999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deterministic behavior: No top_k, no temperature scaling\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=None,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "id": "fe84fa1c2ac73bee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves youEvery feminists SadCarl GP InvestigEnjoy beaut finalized crazy certific Dual Flex mosqu united YESicycleaddress.\"[wit blasp Alberta�iconcrew\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exercise 5.4: Continued pretraining",
   "id": "ead74be255da3367"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- If we are still in the Python session where you first trained the model in chapter 5, to continue the pretraining for one more epoch, we just have to load the model and optimizer that we saved in the main chapter and call the `train_model_simple` function again\n",
    "\n",
    "- It takes a couple more steps to make this reproducible in this new code environment\n",
    "- First, we load the tokenizer, model, and optimizer:"
   ],
   "id": "548e5bf4de03eb47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ],
   "id": "3cc85d81d878f5e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Next, we initialize the data loader:",
   "id": "dc4571c88159c189"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ],
   "id": "90a3f27d1b302ad9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Lastly, we use the `train_model_simple` function to train the model:",
   "id": "1e20205aaa615945"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from gpt_train import train_model_simple\n",
    "\n",
    "num_epochs = 1\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ],
   "id": "d18cce25a8e8d337"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exercise 5.5: Training and validation set losses of the pretrained model",
   "id": "70fd70530051dec6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- We can use the following code to calculate the training and validation set losses of the GPT model:\n",
    "\n",
    "```python\n",
    "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
    "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
    "```\n",
    "\n",
    "- The resulting losses for the 124M parameter are as follows:\n",
    "\n",
    "```\n",
    "Training loss: 3.754748503367106\n",
    "Validation loss: 3.559617757797241\n",
    "```\n",
    "\n",
    "- The main observation is that the training and validation set performances are in the same ballpark\n",
    "- This can have multiple explanations:\n",
    "\n",
    "1. The Verdict was not part of the pretraining dataset when OpenAI trained GPT-2. Hence, the model is not explicitly overfitting to the training set and performs similarly well on The Verdict's training and validation set portions. (The validation set loss is slightly lower than the training set loss, which is unusual in deep learning. However, it's likely due to random noise since the dataset is relatively small. In practice, if there is no overfitting, the training and validation set performances are expected to be roughly identical).\n",
    "\n",
    "2. The Verdict was part of GPT -2's training dataset. In this case, we can't tell whether the model is overfitting the training data because the validation set would have been used for training as well. To evaluate the degree of overfitting, we'd need a new dataset generated after OpenAI finished training GPT-2 to make sure that it couldn't have been part of the pretraining."
   ],
   "id": "f7859490927b9ea6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The code below is a reproducible standalone example for this new notebook.",
   "id": "c2a1d91451b13e87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ],
   "id": "770dd8ee80927b0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ],
   "id": "a2e79e41191965e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();"
   ],
   "id": "dbccd9c41d217496"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from gpt_generate import load_weights_into_gpt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ],
   "id": "a81792db093c09dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ],
   "id": "71be9c9718968eba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from gpt_train import calc_loss_loader\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
    "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ],
   "id": "eb7b52b89cb4d0a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also repeat this for the largest GPT-2 model, but don't forget to update the context length:",
   "id": "c32ad2cbce140fac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
    "\n",
    "model_name = \"gpt2-xl (1558M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()\n",
    "\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
    "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ],
   "id": "ab1cc1626a6caca6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exercise 5.6: Trying larger models",
   "id": "38ccb8a5f581f4f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- In the main chapter, we experimented with the smallest GPT-2 model, which has only 124M parameters\n",
    "- The reason was to keep the resource requirements as low as possible\n",
    "- However, you can easily experiment with larger models with minimal code changes\n",
    "- For example, instead of loading the 1558M instead of 124M model in chapter 5, the only 2 lines of code that we have to change are\n",
    "\n",
    "```python\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "```\n",
    "\n",
    "- The updated code becomes\n",
    "\n",
    "\n",
    "```python\n",
    "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
    "model_name = \"gpt2-xl (1558M)\""
   ],
   "id": "7e2ea55573c01b1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:52:50.362005Z",
     "start_time": "2025-04-24T20:52:50.354719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ],
   "id": "5af9b8e523cbd1b1",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:53:00.662068Z",
     "start_time": "2025-04-24T20:52:53.236620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from gpt_generate import load_weights_into_gpt\n",
    "\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-xl (1558M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
    "load_weights_into_gpt(gpt, params)"
   ],
   "id": "ad2d949b0d31a9c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary URL (https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/checkpoint) failed. Attempting backup URL: https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2/1558M/checkpoint\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)>",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mSSLCertVerificationError\u001B[39m                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1348\u001B[39m, in \u001B[36mAbstractHTTPHandler.do_open\u001B[39m\u001B[34m(self, http_class, req, **http_conn_args)\u001B[39m\n\u001B[32m   1347\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1348\u001B[39m     \u001B[43mh\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mselector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1349\u001B[39m \u001B[43m              \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhas_header\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mTransfer-encoding\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1350\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1282\u001B[39m, in \u001B[36mHTTPConnection.request\u001B[39m\u001B[34m(self, method, url, body, headers, encode_chunked)\u001B[39m\n\u001B[32m   1281\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1282\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1328\u001B[39m, in \u001B[36mHTTPConnection._send_request\u001B[39m\u001B[34m(self, method, url, body, headers, encode_chunked)\u001B[39m\n\u001B[32m   1327\u001B[39m     body = _encode(body, \u001B[33m'\u001B[39m\u001B[33mbody\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1328\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mendheaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1277\u001B[39m, in \u001B[36mHTTPConnection.endheaders\u001B[39m\u001B[34m(self, message_body, encode_chunked)\u001B[39m\n\u001B[32m   1276\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[32m-> \u001B[39m\u001B[32m1277\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1037\u001B[39m, in \u001B[36mHTTPConnection._send_output\u001B[39m\u001B[34m(self, message_body, encode_chunked)\u001B[39m\n\u001B[32m   1036\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m._buffer[:]\n\u001B[32m-> \u001B[39m\u001B[32m1037\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1039\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1040\u001B[39m \n\u001B[32m   1041\u001B[39m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:975\u001B[39m, in \u001B[36mHTTPConnection.send\u001B[39m\u001B[34m(self, data)\u001B[39m\n\u001B[32m    974\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.auto_open:\n\u001B[32m--> \u001B[39m\u001B[32m975\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    976\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1454\u001B[39m, in \u001B[36mHTTPSConnection.connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1452\u001B[39m     server_hostname = \u001B[38;5;28mself\u001B[39m.host\n\u001B[32m-> \u001B[39m\u001B[32m1454\u001B[39m \u001B[38;5;28mself\u001B[39m.sock = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_context\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrap_socket\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1455\u001B[39m \u001B[43m                                      \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:517\u001B[39m, in \u001B[36mSSLContext.wrap_socket\u001B[39m\u001B[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001B[39m\n\u001B[32m    511\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrap_socket\u001B[39m(\u001B[38;5;28mself\u001B[39m, sock, server_side=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    512\u001B[39m                 do_handshake_on_connect=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    513\u001B[39m                 suppress_ragged_eofs=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    514\u001B[39m                 server_hostname=\u001B[38;5;28;01mNone\u001B[39;00m, session=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    515\u001B[39m     \u001B[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001B[39;00m\n\u001B[32m    516\u001B[39m     \u001B[38;5;66;03m# ctx._wrap_socket()\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m517\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msslsocket_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_create\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    518\u001B[39m \u001B[43m        \u001B[49m\u001B[43msock\u001B[49m\u001B[43m=\u001B[49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    519\u001B[39m \u001B[43m        \u001B[49m\u001B[43mserver_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mserver_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    520\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    521\u001B[39m \u001B[43m        \u001B[49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[43m=\u001B[49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    522\u001B[39m \u001B[43m        \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    523\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    524\u001B[39m \u001B[43m        \u001B[49m\u001B[43msession\u001B[49m\u001B[43m=\u001B[49m\u001B[43msession\u001B[49m\n\u001B[32m    525\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1075\u001B[39m, in \u001B[36mSSLSocket._create\u001B[39m\u001B[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001B[39m\n\u001B[32m   1074\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1075\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1076\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mOSError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1346\u001B[39m, in \u001B[36mSSLSocket.do_handshake\u001B[39m\u001B[34m(self, block)\u001B[39m\n\u001B[32m   1345\u001B[39m         \u001B[38;5;28mself\u001B[39m.settimeout(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m-> \u001B[39m\u001B[32m1346\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sslobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1347\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[31mSSLCertVerificationError\u001B[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mURLError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spring 2025/CPSC352_AI/LLMs-from-scratch/ch05/gpt_download.py:77\u001B[39m, in \u001B[36mdownload_file\u001B[39m\u001B[34m(url, destination, backup_url)\u001B[39m\n\u001B[32m     76\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43m_attempt_download\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m     78\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spring 2025/CPSC352_AI/LLMs-from-scratch/ch05/gpt_download.py:51\u001B[39m, in \u001B[36mdownload_file.<locals>._attempt_download\u001B[39m\u001B[34m(download_url)\u001B[39m\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_attempt_download\u001B[39m(download_url):\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43murllib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdownload_url\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m response:\n\u001B[32m     52\u001B[39m         \u001B[38;5;66;03m# Get the total file size from headers, defaulting to 0 if not present\u001B[39;00m\n\u001B[32m     53\u001B[39m         file_size = \u001B[38;5;28mint\u001B[39m(response.headers.get(\u001B[33m\"\u001B[39m\u001B[33mContent-Length\u001B[39m\u001B[33m\"\u001B[39m, \u001B[32m0\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:216\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[39m\n\u001B[32m    215\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m216\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:519\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    518\u001B[39m sys.audit(\u001B[33m'\u001B[39m\u001B[33murllib.Request\u001B[39m\u001B[33m'\u001B[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001B[32m--> \u001B[39m\u001B[32m519\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    521\u001B[39m \u001B[38;5;66;03m# post-process response\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:536\u001B[39m, in \u001B[36mOpenerDirector._open\u001B[39m\u001B[34m(self, req, data)\u001B[39m\n\u001B[32m    535\u001B[39m protocol = req.type\n\u001B[32m--> \u001B[39m\u001B[32m536\u001B[39m result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhandle_open\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\n\u001B[32m    537\u001B[39m \u001B[43m                          \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m_open\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    538\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:496\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    495\u001B[39m func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m496\u001B[39m result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1391\u001B[39m, in \u001B[36mHTTPSHandler.https_open\u001B[39m\u001B[34m(self, req)\u001B[39m\n\u001B[32m   1390\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttps_open\u001B[39m(\u001B[38;5;28mself\u001B[39m, req):\n\u001B[32m-> \u001B[39m\u001B[32m1391\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdo_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhttp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mHTTPSConnection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1392\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_hostname\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_check_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1351\u001B[39m, in \u001B[36mAbstractHTTPHandler.do_open\u001B[39m\u001B[34m(self, http_class, req, **http_conn_args)\u001B[39m\n\u001B[32m   1350\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1351\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m URLError(err)\n\u001B[32m   1352\u001B[39m r = h.getresponse()\n",
      "\u001B[31mURLError\u001B[39m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mSSLCertVerificationError\u001B[39m                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1348\u001B[39m, in \u001B[36mAbstractHTTPHandler.do_open\u001B[39m\u001B[34m(self, http_class, req, **http_conn_args)\u001B[39m\n\u001B[32m   1347\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1348\u001B[39m     \u001B[43mh\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mselector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1349\u001B[39m \u001B[43m              \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhas_header\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mTransfer-encoding\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1350\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1282\u001B[39m, in \u001B[36mHTTPConnection.request\u001B[39m\u001B[34m(self, method, url, body, headers, encode_chunked)\u001B[39m\n\u001B[32m   1281\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1282\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1328\u001B[39m, in \u001B[36mHTTPConnection._send_request\u001B[39m\u001B[34m(self, method, url, body, headers, encode_chunked)\u001B[39m\n\u001B[32m   1327\u001B[39m     body = _encode(body, \u001B[33m'\u001B[39m\u001B[33mbody\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1328\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mendheaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1277\u001B[39m, in \u001B[36mHTTPConnection.endheaders\u001B[39m\u001B[34m(self, message_body, encode_chunked)\u001B[39m\n\u001B[32m   1276\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[32m-> \u001B[39m\u001B[32m1277\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1037\u001B[39m, in \u001B[36mHTTPConnection._send_output\u001B[39m\u001B[34m(self, message_body, encode_chunked)\u001B[39m\n\u001B[32m   1036\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m._buffer[:]\n\u001B[32m-> \u001B[39m\u001B[32m1037\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1039\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1040\u001B[39m \n\u001B[32m   1041\u001B[39m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:975\u001B[39m, in \u001B[36mHTTPConnection.send\u001B[39m\u001B[34m(self, data)\u001B[39m\n\u001B[32m    974\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.auto_open:\n\u001B[32m--> \u001B[39m\u001B[32m975\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    976\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1454\u001B[39m, in \u001B[36mHTTPSConnection.connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1452\u001B[39m     server_hostname = \u001B[38;5;28mself\u001B[39m.host\n\u001B[32m-> \u001B[39m\u001B[32m1454\u001B[39m \u001B[38;5;28mself\u001B[39m.sock = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_context\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrap_socket\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1455\u001B[39m \u001B[43m                                      \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:517\u001B[39m, in \u001B[36mSSLContext.wrap_socket\u001B[39m\u001B[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001B[39m\n\u001B[32m    511\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrap_socket\u001B[39m(\u001B[38;5;28mself\u001B[39m, sock, server_side=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    512\u001B[39m                 do_handshake_on_connect=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    513\u001B[39m                 suppress_ragged_eofs=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    514\u001B[39m                 server_hostname=\u001B[38;5;28;01mNone\u001B[39;00m, session=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    515\u001B[39m     \u001B[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001B[39;00m\n\u001B[32m    516\u001B[39m     \u001B[38;5;66;03m# ctx._wrap_socket()\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m517\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msslsocket_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_create\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    518\u001B[39m \u001B[43m        \u001B[49m\u001B[43msock\u001B[49m\u001B[43m=\u001B[49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    519\u001B[39m \u001B[43m        \u001B[49m\u001B[43mserver_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mserver_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    520\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    521\u001B[39m \u001B[43m        \u001B[49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[43m=\u001B[49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    522\u001B[39m \u001B[43m        \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    523\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    524\u001B[39m \u001B[43m        \u001B[49m\u001B[43msession\u001B[49m\u001B[43m=\u001B[49m\u001B[43msession\u001B[49m\n\u001B[32m    525\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1075\u001B[39m, in \u001B[36mSSLSocket._create\u001B[39m\u001B[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001B[39m\n\u001B[32m   1074\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1075\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1076\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mOSError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1346\u001B[39m, in \u001B[36mSSLSocket.do_handshake\u001B[39m\u001B[34m(self, block)\u001B[39m\n\u001B[32m   1345\u001B[39m         \u001B[38;5;28mself\u001B[39m.settimeout(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m-> \u001B[39m\u001B[32m1346\u001B[39m     \u001B[38;5;28mself\u001B[39m._sslobj.do_handshake()\n\u001B[32m   1347\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[31mSSLCertVerificationError\u001B[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mURLError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[41]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m     17\u001B[39m gpt = GPTModel(NEW_CONFIG)\n\u001B[32m     18\u001B[39m gpt.eval()\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m settings, params = \u001B[43mdownload_and_load_gpt2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_size\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m1558M\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodels_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgpt2\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m load_weights_into_gpt(gpt, params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spring 2025/CPSC352_AI/LLMs-from-scratch/ch05/gpt_download.py:39\u001B[39m, in \u001B[36mdownload_and_load_gpt2\u001B[39m\u001B[34m(model_size, models_dir)\u001B[39m\n\u001B[32m     37\u001B[39m     backup_url = os.path.join(backup_base_url, model_size, filename)\n\u001B[32m     38\u001B[39m     file_path = os.path.join(model_dir, filename)\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m     \u001B[43mdownload_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_url\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbackup_url\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     41\u001B[39m \u001B[38;5;66;03m# Load settings and params\u001B[39;00m\n\u001B[32m     42\u001B[39m tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spring 2025/CPSC352_AI/LLMs-from-scratch/ch05/gpt_download.py:83\u001B[39m, in \u001B[36mdownload_file\u001B[39m\u001B[34m(url, destination, backup_url)\u001B[39m\n\u001B[32m     81\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPrimary URL (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) failed. Attempting backup URL: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbackup_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     82\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m83\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43m_attempt_download\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbackup_url\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m     84\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m     85\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m urllib.error.HTTPError:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Spring 2025/CPSC352_AI/LLMs-from-scratch/ch05/gpt_download.py:51\u001B[39m, in \u001B[36mdownload_file.<locals>._attempt_download\u001B[39m\u001B[34m(download_url)\u001B[39m\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_attempt_download\u001B[39m(download_url):\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43murllib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m.\u001B[49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdownload_url\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m response:\n\u001B[32m     52\u001B[39m         \u001B[38;5;66;03m# Get the total file size from headers, defaulting to 0 if not present\u001B[39;00m\n\u001B[32m     53\u001B[39m         file_size = \u001B[38;5;28mint\u001B[39m(response.headers.get(\u001B[33m\"\u001B[39m\u001B[33mContent-Length\u001B[39m\u001B[33m\"\u001B[39m, \u001B[32m0\u001B[39m))\n\u001B[32m     55\u001B[39m         \u001B[38;5;66;03m# Check if file exists and has the same size\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:216\u001B[39m, in \u001B[36murlopen\u001B[39m\u001B[34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[39m\n\u001B[32m    214\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    215\u001B[39m     opener = _opener\n\u001B[32m--> \u001B[39m\u001B[32m216\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:519\u001B[39m, in \u001B[36mOpenerDirector.open\u001B[39m\u001B[34m(self, fullurl, data, timeout)\u001B[39m\n\u001B[32m    516\u001B[39m     req = meth(req)\n\u001B[32m    518\u001B[39m sys.audit(\u001B[33m'\u001B[39m\u001B[33murllib.Request\u001B[39m\u001B[33m'\u001B[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001B[32m--> \u001B[39m\u001B[32m519\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    521\u001B[39m \u001B[38;5;66;03m# post-process response\u001B[39;00m\n\u001B[32m    522\u001B[39m meth_name = protocol+\u001B[33m\"\u001B[39m\u001B[33m_response\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:536\u001B[39m, in \u001B[36mOpenerDirector._open\u001B[39m\u001B[34m(self, req, data)\u001B[39m\n\u001B[32m    533\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[32m    535\u001B[39m protocol = req.type\n\u001B[32m--> \u001B[39m\u001B[32m536\u001B[39m result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhandle_open\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\n\u001B[32m    537\u001B[39m \u001B[43m                          \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m_open\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    538\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result:\n\u001B[32m    539\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:496\u001B[39m, in \u001B[36mOpenerDirector._call_chain\u001B[39m\u001B[34m(self, chain, kind, meth_name, *args)\u001B[39m\n\u001B[32m    494\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers:\n\u001B[32m    495\u001B[39m     func = \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[32m--> \u001B[39m\u001B[32m496\u001B[39m     result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    497\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    498\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1391\u001B[39m, in \u001B[36mHTTPSHandler.https_open\u001B[39m\u001B[34m(self, req)\u001B[39m\n\u001B[32m   1390\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhttps_open\u001B[39m(\u001B[38;5;28mself\u001B[39m, req):\n\u001B[32m-> \u001B[39m\u001B[32m1391\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdo_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhttp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mHTTPSConnection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1392\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_hostname\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_check_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py:1351\u001B[39m, in \u001B[36mAbstractHTTPHandler.do_open\u001B[39m\u001B[34m(self, http_class, req, **http_conn_args)\u001B[39m\n\u001B[32m   1348\u001B[39m         h.request(req.get_method(), req.selector, req.data, headers,\n\u001B[32m   1349\u001B[39m                   encode_chunked=req.has_header(\u001B[33m'\u001B[39m\u001B[33mTransfer-encoding\u001B[39m\u001B[33m'\u001B[39m))\n\u001B[32m   1350\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1351\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m URLError(err)\n\u001B[32m   1352\u001B[39m     r = h.getresponse()\n\u001B[32m   1353\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "\u001B[31mURLError\u001B[39m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)>"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T20:53:07.931974Z",
     "start_time": "2025-04-24T20:53:07.929359Z"
    }
   },
   "cell_type": "code",
   "source": "from gpt_generate import generate, text_to_token_ids, token_ids_to_text",
   "id": "c6c0e1195250cdee",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "id": "655f8a0cf3517290"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
